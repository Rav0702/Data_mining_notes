{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dimensionality Reduction\n",
    "The technique used to reduce the number of features in a dataset while retaining as much information as possible is called dimensionality reduction. Dimensionality reduction is a key step in the data preprocessing pipeline. It can be used to reduce the number of features in a dataset, which can be useful for the following reasons:\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "Principal Component Analysis (PCA) is a technique used to transform a dataset into a new coordinate system where the dimensions are orthogonal and uncorrelated. Simply put, PCA is a mathematical method (orthogonal linear transformation) that takes a dataset and reduces its dimensions while retaining as much of the original information as possible. This technique is commonly used to simplify complex data sets and make them easier to visualize or work with.\n",
    "PCA works by identifying the most important variables in a dataset and creating a new set of variables that captures as much of the variance in the original data as possible. The new set of variables, known as Principal Components, are ordered in terms of their importance, with the first Principal Component explaining the most variance in the data, the second explaining the second-most, and so on.\n",
    "\n",
    "### Assumptions\n",
    "- the relationship between the variables/features are linear\n",
    "- the direction with the largest variance is the most informative\n",
    "- the principle components are orthogonal (linearly independent, linearly uncorrelated)\n",
    "\n",
    "### Advantages\n",
    "- Fast\n",
    "- Easy to interpret\n",
    "- Works well with linear models (linear correlation)\n",
    "\n",
    "### Disadvantages\n",
    "- Assumes linear relationship between variables (strict assumptions)\n",
    "- Cannot capture non-linear relationships\n",
    "- Cannot capture complex relationships\n",
    "\n",
    "Intrinsic dimension is the minimum number of variables needed to represent the key features of a dataset. It is not necessarily the same as the number of variables in the original dataset. Knowing the intrinsic dimension can help choose the right dimensionality reduction technique for a dataset.\n",
    "\n",
    "## Manifold Learning\n",
    "Manifold Learning is another technique used in dimensionality reduction. Unlike PCA, which is a linear method, manifold learning is a nonlinear method that can capture complex relationships between variables in a dataset.\n",
    "\n",
    "Manifold learning works by mapping the dataset to a lower-dimensional space, while still preserving the relationships between the variables. The lower-dimensional space is often a manifold, which is a mathematical object that can be thought of as a curved surface embedded in higher-dimensional space. By mapping the dataset to a lower-dimensional manifold, it becomes easier to visualize and work with.\n",
    "\n",
    "Manifold learning “estimates” this low-dimensional manifold, typically based only on the distances between the high-dimensional data.\n",
    "\n",
    "## t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear technique used in dimensionality reduction, similar to manifold learning. t-SNE is particularly useful for visualizing high-dimensional data in a low-dimensional space, such as a 2D or 3D plot.\n",
    "\n",
    "t-SNE works by modeling the high-dimensional data using a probability distribution, and then modeling the low-dimensional data using another probability distribution. The goal is to minimize the divergence between the two probability distributions, which is achieved by adjusting the positions of the data points in the low-dimensional space.\n",
    "\n",
    "### Crowding problem\n",
    "When attempting to preserve medium-range distances between data points, we also attempt to preserve the volume spanned by them. There isn't enough area/volume available in lower-dimensional spaces to reliably represent very large volumes in high-dimensional space. So when we \"pull together\" more distant points, we are overcrowding densely populated areas in low dimensional space and cannot separate distinct clusters very well.\n",
    "\n",
    "### Advantages\n",
    "- Can capture complex relationships (non-linear)\n",
    "- Effective for visualizing high-dimensional data\n",
    "\n",
    "### Disadvantages\n",
    "- Slow for large datasets\n",
    "- Parameters are difficult to tune\n",
    "\n",
    "## UMAP\n",
    "UMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique that is similar to t-SNE, but is faster and more scalable. UMAP can be used to visualize high-dimensional data in a low-dimensional space, as well as for clustering and classification tasks.\n",
    "\n",
    "UMAP works by constructing a weighted graph based on the high-dimensional data, and then optimizing a low-dimensional representation of the data that preserves the graph structure. UMAP is particularly effective at preserving the global structure of the data, while still being able to capture local structure and details.\n",
    "\n",
    "### Advantages\n",
    "- Fast and scalable\n",
    "- Preserves global structure of the data\n",
    "- Good on large datasets\n",
    "\n",
    "### Disadvantages\n",
    "- Need to handpick the free parameters\n",
    "- Sizes and distances between each cluster are not informative\n",
    "- Sensitive to optimazation parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
