{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Distances\n",
    "## Distance metric\n",
    "A distance metric is a function that measures the distance between two data sets / points.\n",
    "a distance metric d(...) is a function that satisfies the following properties:\n",
    "1. d(x, y) ≥ 0 non-negativity\n",
    "2. d(x, y) = 0 if and only if x = y identity of indiscernibles\n",
    "3. d(x, y) = d(y, x) symmetry\n",
    "4. d(x, y) + d(y, z) ≥ d(x, z) triangle inequality\n",
    "\n",
    "## Jaccard distance - sets\n",
    "The Jaccard distance between two sets is defined as the size of the intersection divided by the size of the union of the two sets.\n",
    "$$d_{J}(A,B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "Applications:\n",
    "- Text similarity\n",
    "- Set similarity\n",
    "- Image similarity\n",
    "- finding similar pieces of malware\n",
    "- Collaborative Filtering - Amazon recs\n",
    "- ...\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def jaccard_similarity(A: set, B: set) -> float:\n",
    "  union = A.union(B)\n",
    "  intersection = A.intersection(B)\n",
    "  return len(intersection)/len(union)\n",
    "def jaccard_distance(A: set, B: set) -> float:\n",
    "  return 1 - jaccard_similarity(A, B)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Euclidean distance - vectors\n",
    "The Euclidean distance between two vectors is the length of the line segment connecting them. (lower bound on the distance between two points in a grid)\n",
    "$$d_{E}(A,B) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}$$\n",
    "When to use:\n",
    "- Continuous data\n",
    "- not too many colums\n",
    "- Magnitude matters ???\n",
    "When to avoid:\n",
    "- Discrete data\n",
    "- Many columns\n",
    "- Sparse data -> Zero values are treated as any other\n",
    "- high dimensionality -> Curse of dimensionality\n",
    "- When magnitude matters (It is not scale invariant! (dist(a,b) ≠ dist(ac,bc) for constant c))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def euclidean_distance(a: np.array, b: np.array) -> float:\n",
    "  return np.sqrt(np.sum(np.power(a - b, 2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Manhattan distance - vectors\n",
    "The Manhattan distance between two vectors is the sum of the absolute differences of their coordinates. (upper bound on the distance between two points in a grid)\n",
    "$$d_{M}(A,B) = \\sum_{i=1}^{n} |a_i - b_i|$$\n",
    "When to use:\n",
    "- when outliers are a problem\n",
    "- Incomaparable features\n",
    "- Many dimentions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Minkowski distance - vectors (generalization of Euclidean and Manhattan)\n",
    "The Minkowski distance between two vectors is the length of the line segment connecting them.\n",
    "$$d_{M}(A,B) = \\sqrt[p]{\\sum_{i=1}^{n} (a_i - b_i)^p}$$\n",
    "\n",
    "Generalized Minkwoski distance:\n",
    "- L0 = Non-zeros\n",
    "- L1 = Manhattan distance\n",
    "- L2 = Euclidean distance\n",
    "- Linf = Maximum distance / chebyshev\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lp_norm(a: np.array, b: np.array, p : int) -> float:\n",
    "  return np.power(np.sum(np.power(np.abs(a - b), p)), 1/p)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Maximum (Chebyshev) distance - vectors\n",
    "The Maximum distance between two vectors is the maximum difference of their coordinates.\n",
    "$$d_{M}(A,B) = max(|a_i - b_i|)$$\n",
    "When to use:\n",
    "- Incomparable features\n",
    "When to avoid:\n",
    "- when outliers are a problem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hamming distance - vectors\n",
    "The Hamming distance between two vectors is the number of positions at which the corresponding symbols are different.\n",
    "$$d_{H}(A,B) = \\sum_{i=1}^{n} |a_i - b_i|$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hamming_distance(a: np.array, b: np.array) -> int:\n",
    "  return np.sum(a != b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cosine distance - vectors\n",
    "The Cosine distance between two vectors is the cosine of the angle between them. Not a metric (violates triangle ineq). Scale invariant but not translation invariant.\n",
    "$$d_{C}(A,B) = 1 - \\frac{A \\cdot B}{|A| |B|}$$\n",
    "When to use:\n",
    "- bag of words (distances do not depend on length of document)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cosine_distance(a: np.array, b: np.array) -> float:\n",
    "  return 1 - (np.dot(a,b)/(np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "a = np.array([1,1])\n",
    "b = np.array([1,0])\n",
    "c = np.array([0,1])\n",
    "\n",
    "dist_ab = cosine_distance(a,b)\n",
    "dist_bc = cosine_distance(b,c)\n",
    "dist_ac = cosine_distance(a,c)\n",
    "print(dist_bc, dist_ab, dist_ac)\n",
    "print(dist_bc > dist_ab + dist_ac)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ISOMAP Distance - vectors\n",
    "Compute kNN for each data point → Construct weighted graph: weights = distances → Set : ISOMAP distance(p,q) = weight of shortest path from p to q."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import heapq as heap\n",
    "from collections import defaultdict\n",
    "\n",
    "def dijkstra(G, startingNode):\n",
    "\tvisited = set()\n",
    "\tparentsMap = {}\n",
    "\tpq = []\n",
    "\tnodeCosts = defaultdict(lambda: float('inf'))\n",
    "\tnodeCosts[startingNode] = 0\n",
    "\theap.heappush(pq, (0, startingNode))\n",
    "\n",
    "\twhile pq:\n",
    "\t\t_, node = heap.heappop(pq)\n",
    "\t\tvisited.add(node)\n",
    "\n",
    "\t\tfor weight, adjNode  in G[node]:\n",
    "\t\t\tif adjNode in visited:\tcontinue\n",
    "\n",
    "\t\t\tnewCost = nodeCosts[node] + weight\n",
    "\t\t\tif nodeCosts[adjNode] > newCost:\n",
    "\t\t\t\tparentsMap[adjNode] = node\n",
    "\t\t\t\tnodeCosts[adjNode] = newCost\n",
    "\t\t\t\theap.heappush(pq, (newCost, adjNode))\n",
    "\n",
    "\treturn parentsMap, nodeCosts\n",
    "def ISOMAP_distance(a_index: int, b_index: int, dataset: np.array, k: int) -> float:\n",
    "  def knn_for_one_point(p_index : int, data: np.array) -> np.array:\n",
    "    return sorted([(euclidean_distance(data[p_index], vector), index) for index,vector in enumerate(data) if index != p_index])[:k]\n",
    "  all_knns = [knn_for_one_point(index, dataset) for index, _ in enumerate(dataset)]\n",
    "  return dijkstra(all_knns, a_index)[1][b_index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kullback-Leibler divergence - statistical\n",
    "The Kullback-Leibler divergence between two probability distributions is a measure of how one probability distribution is different from a second, reference probability distribution.\n",
    "$$D_{KL}(P||Q) = \\sum_{i=1}^{n} P(i) \\log \\frac{P(i)}{Q(i)}$$ -> discrete\n",
    "$$D_{KL}(P||Q) = \\int_{-\\infty}^{\\infty} P(x) \\log \\frac{P(x)}{Q(x)} dx$$ -> continuous\n",
    "\n",
    "- Measures \"surprise\" of the distribution P with respect to Q. (P is the true distribution, Q is the estimated distribution)\n",
    "- The expected value of log likelyhood ratio between P and Q.\n",
    "- The number of bits needed to encode a sample from P, assuming a Q model.\n",
    "- Information gain or relative entropy\n",
    "- Not a metric (not symmetric)\n",
    "\n",
    "Forward KL P(P||Q) when P is high, Q should be high\n",
    "![Row normalization effect](./assets/kl.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def discrete_kl_divergence(P: np.array, Q: np.array) -> float:\n",
    "  return np.sum(P * np.log2(P/Q))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discrete_kl_divergence(\n",
    "    np.array([0.01, 0.1, 0.39, 0.5]),\n",
    "    np.array([0.1, 0.5, 0.3, 0.1]),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discrete_kl_divergence(\n",
    "    np.array([0.1, 0.5, 0.3, 0.1]),\n",
    "    np.array([0.01, 0.1, 0.39, 0.5])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Jensen-Shannon divergence - statistical\n",
    "The Jensen-Shannon divergence between two probability distributions is a measure of how one probability distribution is different from a second, reference probability distribution.\n",
    "$$D_{JS}(P||Q) = \\frac{1}{2} D_{KL}(P||M) + \\frac{1}{2} D_{KL}(Q||M)$$ with $M = \\frac{1}{2}(P+Q)$\n",
    "- Unlike KL divergence, JS divergence is symmetric and is a metric.\n",
    "- It will not be infinite if one of the distributions is zero."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def jensen_shannon_divergence(P: np.array, Q: np.array) -> float:\n",
    "  M = (P + Q)/2\n",
    "  return (discrete_kl_divergence(P, M) + discrete_kl_divergence(Q, M))/2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jensen_shannon_divergence(\n",
    "    np.array([0.1, 0.5, 0.3, 0.1]),\n",
    "    np.array([0.01, 0.1, 0.39, 0.5])\n",
    ") == jensen_shannon_divergence(\n",
    "    np.array([0.01, 0.1, 0.39, 0.5]),\n",
    "    np.array([0.1, 0.5, 0.3, 0.1]),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dynamic Time Warping DTW - sequential\n",
    "Dynamic time warping is another distance measure for sequential data that allows for differences in the speed and timing of the sequences. It is commonly used in speech recognition, image matching, and time series analysis.\n",
    "Not a metrics, does not satisfy triangle inequality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dynamic_time_warping(a: np.array, b: np.array) -> float:\n",
    "  distance_matrix = np.array([[euclidean_distance(i,j)**2 for j in b] for i in a])\n",
    "  result_matrix = np.zeros(distance_matrix.shape)\n",
    "  for i in range(len(a)):\n",
    "    for j in range(len(b)):\n",
    "      minimum_path = 100000\n",
    "      if i != 0:\n",
    "        minimum_path = min(minimum_path, result_matrix[i-1,j])\n",
    "      if j != 0:\n",
    "        minimum_path = min(minimum_path, result_matrix[i,j-1])\n",
    "      if i != 0 and j != 0:\n",
    "        minimum_path = min(minimum_path, result_matrix[i-1,j-1])\n",
    "      if minimum_path != 100000:\n",
    "        result_matrix[i,j] = minimum_path + distance_matrix[i,j]\n",
    "      else:\n",
    "        result_matrix[i,j] = distance_matrix[i,j]\n",
    "  return np.sqrt(result_matrix[len(a)-1, len(b)-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Levenshtein distance - sequential\n",
    "Levenshtein distance, also known as edit distance, is a measure of the minimum number of insertions, deletions, and substitutions required to transform one sequence into another. It is commonly used in natural language processing and spell-checking applications.\n",
    "Applications\n",
    "It is a metric\n",
    "Applications:\n",
    "- Spell checking\n",
    "- DNA sequence alignment\n",
    "\n",
    "## Alignment\n",
    "When working with sequential data, it may be necessary to align the sequences in order to compute a meaningful distance. This can be done using techniques such as dynamic time warping or Needleman-Wunsch alignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sequence_alignment(a: list, b: list, W_l: float, W_r: float) -> float:\n",
    "  result_matrix = np.ones((len(a),len(b))) * 1e7\n",
    "  for i in range(len(a)):\n",
    "    result_matrix[i,0]=0\n",
    "  for j in range(len(b)):\n",
    "    result_matrix[0,j]=0\n",
    "\n",
    "  for i in range(1,len(a)):\n",
    "    for j in range(1,len(b)):\n",
    "      result_matrix[i,j] = max(\n",
    "          0,\n",
    "          result_matrix[i-1,j-1] + a[i]!=b[i],\n",
    "          max([result_matrix[i-k,j] + W_l  for k in range(1, i+1)]),\n",
    "          max([result_matrix[i-k,j] + W_r  for k in range(1, i+1)])\n",
    "      )\n",
    "  return np.sqrt(result_matrix[len(a)-1, len(b)-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
