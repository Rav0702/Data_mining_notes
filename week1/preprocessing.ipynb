{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Power Laws\n",
    "   * Power laws describe mathematical relationships between two variables in the form Y = kX^α, where α is a constant exponent that determines the shape of the curve.\n",
    "   * They are often observed in situations where a few highly connected nodes dominate the system, while the majority have only a few connections.\n",
    "   * Power laws exhibit a long tail, meaning there are relatively few occurrences of very large values that can have a significant impact on the overall distribution.\n",
    "   * They are useful for understanding complex systems and predicting their behavior, but must be used with caution as they may not always apply and can be misinterpreted if not applied correctly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Box-Cox Transformation\n",
    "![Box-Cox transform formula](https://latex.codecogs.com/svg.latex?\\large&space;y(\\lambda)=\\begin{cases}\\frac{(y+\\epsilon)^{\\lambda}-1}{\\lambda},&amp;\\lambda\\neq0\\\\ \\ln(y+\\epsilon),&\\lambda=0\\end{cases})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstats\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pareto\n\u001B[0;32m      6\u001B[0m fig, ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      7\u001B[0m b \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2.62\u001B[39m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import pareto\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "b = 2.62\n",
    "mean, var, skew, kurt = pareto.stats(b, moments='mvsk')\n",
    "x = np.linspace(pareto.ppf(0.01, b),\n",
    "                pareto.ppf(0.99, b), 100)\n",
    "ax.plot(x, pareto.pdf(x, b),\n",
    "       'r-', lw=5, alpha=0.6, label='pareto pdf')\n",
    "\n",
    "vals = pareto.ppf([0.001, 0.5, 0.999], b)\n",
    "r = pareto.rvs(b, size=1000)\n",
    "ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
    "ax.set_xlim([x[0], x[-1]])\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cox(X, lam):\n",
    "  if lam == 0:\n",
    "    return np.log(X)\n",
    "  return (np.power(X, lam) - 1)/lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lam = -3.9\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(box_cox(r, lam), density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quantilie transformation\n",
    "![Quantile transform formula](https://latex.codecogs.com/svg.latex?\\large&space;y_i=\\frac{F^{-1}(x_i)-F^{-1}(0)}{F^{-1}(1)-F^{-1}(0)})\n",
    "1. Rank the data in ascending order.\n",
    "2. Replace each value with its corresponding percentile rank.\n",
    "3. Transform the uniform distribution to the desired distribution using the inverse cumulative distribution function (CDF).\n",
    "4. The resulting transformed variable has the same range and number of observations as the original variable, but with a different distribution.\n",
    "5. Quantile transformation is particularly useful when the original variable has a skewed or non-normal distribution, making it difficult to model using standard statistical methods such as linear regression.\n",
    "6. One potential limitation of quantile transformation is that it can be sensitive to outliers, so it may be necessary to either remove outliers or apply a more robust transformation method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Row normalization\n",
    "![Row normalization formula](https://latex.codecogs.com/svg.latex?\\large&space;X_{ij}=\\frac{X_{ij}}{\\sum_{k=1}^{n}X_{ik}})\n",
    "Even after normalizing features, rows have different sizes. Row normalization “fixes” this by ensuring every row’s norm is the same (i.e., sums to 1, or using a different norm)\n",
    "![Row normalization effect](./assets/row_trans.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Column normalization\n",
    " A method used to scale the values of a dataset so that they fall within a specific range. Unlike row normalization, which scales the values within each row, column normalization scales the values within each column. This can be useful when working with datasets that have features with vastly different scales, as it can help to ensure that each feature is given equal weight in the analysis.\n",
    " ## Min-Max Normalization\n",
    "![Min-Max Normalization formula](https://latex.codecogs.com/svg.latex?\\large&space;X_{ij}=\\frac{X_{ij}-\\min(X_{ij})}{\\max(X_{ij})-\\min(X_{ij})})\n",
    " ## Z-Score Normalization\n",
    "![Z-Score Normalization formula](https://latex.codecogs.com/svg.latex?\\large&space;X_{ij}=\\frac{X_{ij}-\\mu}{\\sigma})\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bag of Words\n",
    "The bag-of-words (BoW) model is a common technique in natural language processing for representing text data as a collection of word counts. It involves creating a vocabulary of unique words from a corpus of text documents and representing each document as a vector of word frequencies. The BoW model can be used for tasks such as document classification, information retrieval, and sentiment analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "    document = document.lower()\n",
    "    words = re.findall(r'\\w+', document)\n",
    "    bow = defaultdict(lambda: 0)\n",
    "    for word in words:\n",
    "        bow[word] += 1\n",
    "    return dict(bow)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word embeddings\n",
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are an improvement over simpler bag-of-words model representations that assign a one-hot encoded vector of a fixed size to each word, where the vector is all zeros except for the index of the word, which is marked with a 1. Word embeddings are learned from text data and are comprised of a floating point vector of a fixed size. The values in the vector are learned in such a way that words that have similar meaning will have a similar representation. Word embeddings can be learned using a variety of techniques, such as neural word embedding models like word2vec or GloVe.\n",
    "## Word2Vec\n",
    "Word2Vec is a popular technique for representing words as high-dimensional vectors in natural language processing. It uses a neural network to learn distributed representations of words based on their co-occurrence patterns in a large corpus of text. The resulting vector space can capture semantic relationships between words and be used for tasks such as language modeling, named entity recognition, and machine translation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature selection\n",
    "* Key ideas:\n",
    "1. choose features that correlate with the class\n",
    "2. choose features that explain a large part of the data variance\n",
    "3. choose those that do not correlate with already selected ones\n",
    "4. choose those that lead to the best performance\n",
    "* Selection can use greedy or optimization algorithms\n",
    "* All require some kind of stopping condition/parameter\n",
    "* A common set-up is to use a search-wrapper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encodings\n",
    "## Label encoding\n",
    "a method used in data preprocessing to convert categorical variables into\n",
    "numerical values. Each unique category is assigned a unique integer value, allowing the data to\n",
    "be more easily processed by machine learning algorithms. However, it is important to note that\n",
    "label encoding does not capture any inherent relationships between the categories, and can\n",
    "sometimes lead to incorrect assumptions being made about the data.\n",
    "## Target encoding\n",
    "Sorting features according to their positive class probability can be a useful way to perform\n",
    "target encoding. This involves calculating the probability of the positive class for each category\n",
    "in the categorical variable and sorting the categories based on this probability. The categories\n",
    "with the highest probability of belonging to the positive class are assigned higher numerical\n",
    "values, while those with lower probabilities are assigned lower numerical values. This method\n",
    "can be effective in cases where the relationship between the categorical variable and the target\n",
    "variable is complex, as it allows the machine learning algorithm to capture this complexity.\n",
    "## One hoc encoding\n",
    "In machine learning, one-hot encoding is a technique used to convert categorical data into a\n",
    "format that can be more easily processed by machine learning algorithms. It works by creating a\n",
    "binary vector for each category in the data, with a 1 indicating that the category is present and a\n",
    "0 indicating that it is not. This allows the machine learning algorithm to treat each category as a\n",
    "separate feature, rather than trying to interpret the categories as a single numerical value. OnePreprocessing\n",
    "5\n",
    "hot encoding is particularly useful when working with categorical data that has a large number of\n",
    "categories, as it can help to reduce the dimensionality of the data and improve the performance\n",
    "of the machine learning algorithm.\n",
    "## Binning\n",
    "Binning is a data transformation method that involves dividing a continuous variable into a set\n",
    "of discrete bins or intervals. This can be useful for reducing the impact of outliers, simplifying\n",
    "complex data, and making it easier to compare data across different groups or categories.\n",
    "Binning can be done using a variety of methods, such as equal width or equal frequency\n",
    "binning. However, it is important to choose the appropriate bin size and method carefully, as this\n",
    "can significantly impact the results of any subsequent analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature manipulation\n",
    "## Feature addition\n",
    "Feature addition involves adding new features to a dataset that may improve the performance of\n",
    "machine learning algorithms. These new features could be derived from existing features, or they\n",
    "could be completely new features that are relevant to the problem being solved. Feature addition\n",
    "can be a useful technique when working with datasets that have low dimensionality or that do not\n",
    "contain enough information to accurately predict the target variable.\n",
    "Feature Extraction\n",
    "## Feature extraction\n",
    "Feature extraction involves transforming raw data into a set of features that can be used in\n",
    "machine learning algorithms. This can involve techniques such as dimensionality reduction,\n",
    "clustering, and principal component analysis. Feature extraction is particularly useful when working\n",
    "with datasets that have high dimensionality or that contain a large amount of noise or irrelevant\n",
    "data. By extracting relevant features, we can improve the performance of machine learning\n",
    "algorithms and make them more efficient at solving the problem at hand.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SMOTE\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a method used in machine learning to\n",
    "deal with imbalanced datasets. It works by generating synthetic data points for the minority class in\n",
    "the dataset, which helps to balance the class distribution and improve the performance of machine\n",
    "learning algorithms. The SMOTE algorithm works by selecting a random minority data point and\n",
    "generating a new data point by interpolating between the nearest minority data points. This process\n",
    "is repeated until the desired level of oversampling has been achieved. SMOTE is a powerful tool for\n",
    "dealing with imbalanced datasets, but it is important to use it carefully and avoid overfitting to the\n",
    "minority class.\n",
    "Steps:\n",
    "1. For every minority instance i\n",
    "2. Randomly select one of the k nearest neighbours x\n",
    "Preprocessing 6\n",
    "3. Compute the vector v(i,x) between i and x\n",
    "*  i + v(i, x) = x\n",
    "4. Randomly select a point p along this vector\n",
    "*  p = i + rand(0, 1) * v(i, x)\n",
    "5. Add p to the minority instances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def smote(minoritySet, nSamples=1, k=5):\n",
    "\tneighbours = np.random.randint(1,k,(nSamples))\n",
    "\tscalars = np.random.rand(nSamples)\n",
    "\tfor i in range(nSamples):\n",
    "\t\tb = np.random.randint(0,len(minoritySet)-1,)\n",
    "\t\tdistances = np.linalg.norm(minoritySet - minoritySet[b,:], axis=1)\n",
    "\t\tasc_neighbours = np.argsort(distances)\n",
    "\t\tnew_point = (minoritySet[asc_neighbours[neighbours[i]]] - minoritySet[b])*scalars[i] + minoritySet[b]\n",
    "\t\tminoritySet = np.append(minoritySet, new_point[np.newaxis, :], 0)\n",
    "\treturn minoritySet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tomek Links\n",
    "A pair of instances from different classes that are closest to each other. They are\n",
    "often used for undersampling in imbalanced datasets, as removing them can improve the separation\n",
    "between classes. The Tomek Links algorithm works by identifying all pairs of instances that are\n",
    "Tomek Links and removing the majority instance from each pair. This can be an effective method for\n",
    "improving the performance of machine learning algorithms on imbalanced datasets, but it is\n",
    "important to use it carefully and avoid removing too many instances.\n",
    "# Missing values\n",
    "Imputation → replacing missing values\n",
    "Hot-deck – replace with values from similar rows\n",
    "Cold-deck – replace with values from another dataset\n",
    "Mean substitution – replace with the mean value of the feature\n",
    "Matrix factorization – reduce the dimension, treating ? as latent\n",
    "Regression/prediction – predict the values form available data\n",
    "Multiple imputation – it is wrong to find a point-estimate.."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
