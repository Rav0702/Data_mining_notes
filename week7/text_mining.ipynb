{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vector Space Model\n",
    "\n",
    "The Vector Space Model is a mathematical model used in information retrieval to represent text documents as vectors in a high-dimensional space. The model is based on the idea that words that appear in the same context tend to have similar meanings. It is used to determine the similarity between documents based on the angle between their corresponding vectors in the space.\n",
    "\n",
    "# One Hot Vectors\n",
    "\n",
    "One Hot Vectors is a type of vector representation used in Natural Language Processing (NLP) to represent text as binary vectors. In this representation, each word in the vocabulary is assigned a unique index and is represented as a vector of zeros except for a one in the position corresponding to its index. This allows for efficient storage and processing of large amounts of text data.\n",
    "\n",
    "- Vocabulary: the set of all possible tokens\n",
    "- Word vectors with all 0s and one 1 at the index of that word\n",
    "- Words independent of each other\n",
    "\n",
    "# The Softmax Function\n",
    "\n",
    "The Softmax function is a mathematical function used in text mining to convert a vector of scores into a probability distribution. In the context of text mining, the input to the Softmax function can be a vector of scores representing the relevance of each document to a given query. The output of the Softmax function is a probability distribution over the documents, which can be used to rank the documents in order of relevance to the query.\n",
    "\n",
    "- Softmax defines a distribution and is also differentiable\n",
    "- It is also a estimate of P(class j | x_i)\n",
    "\n",
    "# The Cross Entropy Loss\n",
    "\n",
    "In text mining, the Cross Entropy Loss is a measure of the difference between the predicted probability distribution and the true probability distribution of a classification model. It is used to train models to predict the probability distribution of class labels for a given input, such as a text document. The Cross Entropy Loss penalizes the model for making incorrect predictions, and encourages it to make more accurate predictions by minimizing the difference between the predicted and true probability distributions.\n",
    "\n",
    "# Sparse vs Dense Representations\n",
    "\n",
    "In text mining, there are two main types of vector representations: sparse and dense. Sparse representations are vectors with mostly zeros and only a few non-zero values, while dense representations are vectors with many non-zero values. Sparse representations are more efficient to store and process, but they may not capture all the nuances and relationships between words in a text. Dense representations, on the other hand, can capture more complex relationships between words, but they require more storage and processing power.\n",
    "\n",
    "# Word2Vec Model\n",
    "\n",
    "The Word2Vec Model is a neural network-based model used in text mining to generate dense vector representations of words. It is based on the idea that words that appear in similar contexts tend to have similar meanings. The model is trained on a large corpus of text to learn the relationships between words and generate vector representations that capture these relationships.\n",
    "\n",
    "- Hidden layers size is much smaller than the original dimensionality\n",
    "- Input and output dimensions are the same size as the vocabulary\n",
    "\n",
    "# Negative Sampling\n",
    "\n",
    "Negative Sampling is a technique used in the Word2Vec model to improve training efficiency. Instead of updating weights for all the non-target words in the vocabulary for each training instance, Negative Sampling randomly samples a small subset of non-target words to update. This reduces the amount of computation required to update the weights and speeds up the training process.\n",
    "\n",
    "# Word Embeddings\n",
    "\n",
    "Word embeddings are a type of dense vector representation used in text mining that captures semantic and syntactic relationships between words. They are generated using machine learning techniques, such as the Word2Vec model, and can be used in various natural language processing tasks, such as sentiment analysis and language translation. Word embeddings have been shown to outperform traditional sparse representations in many tasks, but they require more storage and processing power."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
