{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Frequent Items\n",
    "\n",
    "## terminology\n",
    "- given a support threshold t, then sets of items that occur in at least s buckets are called **frequent itemsets**\n",
    "- for rule L-> R **confidence** is the probablilty of L giver R: $$confidence(L->R) = P(R|L) = \\frac{support(L \\cup R)}{support(L)}$$ Rules with greater confidence are more interesting\n",
    "- **Lift** measures how much better an association rule is at predicting the rule body than one based on entire data:\n",
    "    * For rule L-> R\n",
    "    $$lift(L->R) = \\frac{confidence(L->R)}{support(R)} = \\frac {support(L \\cup R)}{support(L) * support(R)}$$\n",
    "    * We typically want rules with high lift, certainly greater than 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "import random\n",
    "\n",
    "X = [\n",
    "    set(\"mcb\"),\n",
    "    set(\"mpj\"),\n",
    "    set(\"mb\"),\n",
    "    set(\"cj\"),\n",
    "    set(\"mpb\"),\n",
    "    set(\"mcbj\"),\n",
    "    set(\"cbj\"),\n",
    "    set(\"bc\")\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def support(L, X) -> float:\n",
    "  return np.sum([1 if np.all([j in x for j in L]) else 0 for x in X])/len(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confidence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def confidence(L, R, X) -> float:\n",
    "  return support(L + R, X)/ support(L,X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lift"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def lift(L, R, X) -> float:\n",
    "  return confidence(L,R,X)/support(R,X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Association Rules\n",
    "Given itemsets L and R, association rule L-> R (L - antecedent or left-hand-side, R consequent or right-hand-side) This simply means that whenever L occurs R is likely to occur as well\n",
    "\n",
    "## Itemset Lattice\n",
    "The itemset lattice is a special data structure that stores all frequent itemsets in a compact form. It is a directed acyclic graph where each node represents an itemset and the edges represent the inclusion relationship between itemsets. The root node represents the empty set, and each level of the graph represents itemsets of a specific size. The itemset lattice is useful for efficiently calculating frequent itemsets and association rules.\n",
    "[Itemset Lattice](../assets/itemset.png)\n",
    "**Maximal frequent itemsets** are those without any supersets of any other frequent itemset (on the border).\n",
    "**Closed frequent itemsets** are those without supersets with the same support.\n",
    "The itemset lattice can be used to efficiently compute both maximal and closed frequent itemsets.\n",
    "\n",
    "## Downward Closure\n",
    "The downward closure property states that if an itemset is frequent, then all of its subsets must also be frequent. This is because if an itemset is frequent, it means that it appears in at least s baskets, and any subset of it must appear in a subset of those baskets. This property is useful in pruning infrequent itemsets and reducing the search space for frequent itemsets and association rules.\n",
    "\n",
    "## Apriori Algorithm\n",
    "The Apriori algorithm is a method for mining frequent itemsets and generating association rules. It works by iteratively constructing an enumeration tree containing all frequent itemsets, and extending the tree by joining pairs of parents from the itemset lattice that occur in the enumeration tree (are frequent). The algorithm uses the downward closure property to efficiently prune infrequent itemsets and reduce the search space for frequent itemsets and association rules. The Apriori algorithm is designed to limit the need for main memory and is a key algorithm for mining frequent itemsets in large datasets.\n",
    "\n",
    "### Psuedocode\n",
    "```\n",
    "Algotithm Apriori(D, s)\n",
    "    Input: D - a dataset of transactions\n",
    "           s - a support threshold\n",
    "    Output: L - a list of all frequent itemsets\n",
    "    L = []\n",
    "    C1 = generate candidate 1-itemsets from D\n",
    "    L1 = generate frequent 1-itemsets from C1\n",
    "    L = L union L1\n",
    "    k = 2\n",
    "    while Lk-1 != {}\n",
    "        Ck = generate candidate k-itemsets from Lk-1\n",
    "        Lk = generate frequent k-itemsets from Ck\n",
    "        L = L union Lk\n",
    "        k = k + 1\n",
    "    return L\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'b'},\n {'m'},\n {'c'},\n {'j'},\n {'b', 'm'},\n {'b', 'c'},\n {'b', 'm'},\n {'b', 'c'},\n {'c', 'j'},\n {'c', 'j'}]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# This function take the previous steps frequent pairs and generates next pairs.\n",
    "def generate_unions(previous):\n",
    "  return [i.union(j) for i in previous for j in previous if len(i.union(j))==len(i)+1]\n",
    "\n",
    "# This removes the items that have infrequent subsets.\n",
    "def prune_subset(previous, new_item):\n",
    "  subsets  = combinations(new_item, len(new_item)-1)\n",
    "  return len([i for i in subsets if i not in previous]) != 0\n",
    "\n",
    "def apriori(X, threshold: float) -> list:\n",
    "\n",
    "  counts_for_1_item_set = defaultdict(lambda: 0)\n",
    "\n",
    "  for i in X:\n",
    "    for j in i:\n",
    "      counts_for_1_item_set[j]+=1\n",
    "\n",
    "  # We get the 1-item frequent sets.\n",
    "  F = [\n",
    "      [set(i) for i in counts_for_1_item_set if counts_for_1_item_set[i]/len(X) > threshold]\n",
    "  ]\n",
    "\n",
    "\n",
    "  k = 0\n",
    "  while len(F[k]) != 0:\n",
    "\n",
    "    # We get the new pairs.\n",
    "    generated = generate_unions(F[k])\n",
    "\n",
    "    # We remove those that contain subsets that are not frequent.\n",
    "    pruned = [i for i in generated if prune_subset(F[k], i)]\n",
    "\n",
    "    # We remove those with small support.\n",
    "    with_support = [i for i in pruned if support(i, X) > threshold]\n",
    "\n",
    "    # We add the new pairs.\n",
    "    F.append(with_support)\n",
    "    k += 1\n",
    "  return functools.reduce(lambda a,b: [*a, *b], F)\n",
    "apriori(X, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enumeration Tree\n",
    "The Apriori algorithm is a method for mining frequent itemsets and generating association rules. It works by iteratively constructing an enumeration tree containing all frequent itemsets, and extending the tree by joining pairs of parents from the itemset lattice that occur in the enumeration tree (are frequent). The algorithm uses the downward closure property to efficiently prune infrequent itemsets and reduce the search space for frequent itemsets and association rules. The Apriori algorithm is designed to limit the need for main memory and is a key algorithm for mining frequent itemsets in large datasets.\n",
    "\n",
    "## FP-Tree\n",
    "An FP-Tree is a tree-like structure used in the FP-growth algorithm for mining frequent itemsets and generating association rules. It compresses the dataset by storing itemsets in a compact form, and avoids the need for repeated database scans. Each node in the tree represents an item, and the edges between nodes represent the frequency of the item in the dataset. The nodes at the bottom of the tree represent frequent itemsets, and the paths from the root to each node represent the support of the itemset. The FP-Tree is used to efficiently mine frequent itemsets by recursively creating conditional subtrees and projecting the data onto itemsets.\n",
    "\n",
    "## FP-growth Algorithm\n",
    "The FP-growth algorithm is an alternative method for mining frequent itemsets and generating association rules. It uses a hierarchical data structure called an FP-tree to compress the dataset and avoid the need for repeated database scans. The algorithm recursively creates conditional subtrees to extract patterns and projection the data onto itemsets. By doing so, it reduces the search space for frequent itemsets and association rules, making it a useful algorithm for mining large datasets.\n",
    "\n",
    "### Psuedocode\n",
    "```\n",
    "Algorithm FP-growth(D, s)\n",
    "    Input: FP-Tree of frequent items: FPT,\n",
    "    Minimum support: minsup\n",
    "    Current Suffix: P\n",
    "    Output: Frequent itemsets\n",
    "\n",
    "    if FTP is a single path\n",
    "        then determine all combinations C of nodes on the path and report C \\cup P as frequent;\n",
    "    else\n",
    "    for each item i in FTP do begin\n",
    "        report itemset P_i = {i} \\cup P asfrequent;\n",
    "        Use pointers to extract conditional prefix paths dorm FPT containding item il\n",
    "        Readjust counts of prefix paths and remove i;\n",
    "        Remove infrequent items from prefix and reconstruct conditional FP-Tree FPT_i;\n",
    "        if (FPR_i is not empty)\n",
    "            then FP-growth(FPT_i, minsup, P_i);\n",
    "    end\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FP-growth Algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "class Tree:\n",
    "  def __init__(self):\n",
    "    self.children = []\n",
    "    self.character = ''\n",
    "    self.count = 0\n",
    "    self.tempCount = 0\n",
    "    self.father = None\n",
    "class FPGrowth:\n",
    "  def __init__(self, data, threshold):\n",
    "    self.root = Tree()\n",
    "    self.pointers = dict()\n",
    "    self.threshold = threshold\n",
    "    count_items = {j:support(j, data) for i in data for j in i if support(j, data) >= threshold}\n",
    "    self.data = [sorted(list(set([j for j in i if j in count_items])), key=lambda x: count_items[x], reverse=True) for i in data]\n",
    "    for i in self.data:\n",
    "      self.addR(i, self.root)\n",
    "\n",
    "  def addR(self, s, root):\n",
    "    root.count += 1\n",
    "    if len(s) == 0:\n",
    "      return\n",
    "    continuation = [i for i in root.children if i.character == s[0]]\n",
    "    if len(continuation) == 0:\n",
    "      newNode = Tree()\n",
    "      newNode.character = s[0]\n",
    "      newNode.count = 1\n",
    "      newNode.father = root\n",
    "      root.children.append(newNode)\n",
    "      if s[0] in self.pointers:\n",
    "        self.pointers[s[0]].append(newNode)\n",
    "      else:\n",
    "        self.pointers[s[0]] = [newNode]\n",
    "      self.addR(s[1:], newNode)\n",
    "    else:\n",
    "      self.addR(s[1:], continuation[0])\n",
    "\n",
    "  def get_path(self, node):\n",
    "    if node.father == None:\n",
    "      return \"\"\n",
    "    return self.get_path(node.father) + node.character\n",
    "\n",
    "  def get_frequent(self):\n",
    "    conditional_pattern_base = {\n",
    "        i: [(set(self.get_path(j)[:-1]), j.count-1) for j in self.pointers[i] if len(self.get_path(j))>1] for i in self.pointers\n",
    "    }\n",
    "    frequent = []\n",
    "    for i in conditional_pattern_base:\n",
    "      values = conditional_pattern_base[i]\n",
    "      if len(values)==0:\n",
    "        continue\n",
    "      inter = functools.reduce(lambda a,b: a.intersection(b), [j[0] for j in values])\n",
    "      inter.add(i)\n",
    "      support = sum([j[1] for j in values])\n",
    "      generated_patterns = [j for j in powerset(inter) if len(j) > 0]\n",
    "      frequent.extend(generated_patterns)\n",
    "    return set(frequent)\n",
    "data = [\n",
    "    \"EKMNOY\",\n",
    "    \"DEKNOY\",\n",
    "    \"AEKM\",\n",
    "    \"CKMUY\",\n",
    "    \"CEIKOO\"\n",
    "]\n",
    "fp = FPGrowth(data,0.5)\n",
    "fp.get_frequent()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import functools\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# This function take the previous steps frequent pairs and generates next pairs.\n",
    "def generate_unions(previous):\n",
    "  return [i.union(j) for i in previous for j in previous if len(i.union(j))==len(i)+1]\n",
    "\n",
    "# This removes the items that have infrequent subsets.\n",
    "def prune_subset(previous, new_item):\n",
    "  subsets  = combinations(new_item, len(new_item)-1)\n",
    "  return len([i for i in subsets if i not in previous]) != 0\n",
    "\n",
    "def PCY(X, threshold: float, dictionary_size: int) -> list:\n",
    "\n",
    "  counts_for_1_item_set = defaultdict(lambda: 0)\n",
    "  bit_mask = np.zeros((dictionary_size))\n",
    "  hash_function = lambda x, y: (hash(y) + hash(x)) % dictionary_size\n",
    "\n",
    "  # This is the place where we now also do the bitmask.\n",
    "  for i in X:\n",
    "    for j in i:\n",
    "      counts_for_1_item_set[j]+=1\n",
    "    for j in i:\n",
    "      for k in i:\n",
    "        if len(set([j,k])) == 2:\n",
    "          bit_mask[hash_function(j,k)]+=1\n",
    "\n",
    "  bit_mask /= len(X)\n",
    "  bit_mask = bit_mask > threshold\n",
    "\n",
    "  # We get the 1-item frequent sets.\n",
    "  F = [\n",
    "      [set(i) for i in counts_for_1_item_set if counts_for_1_item_set[i]/len(X) > threshold]\n",
    "  ]\n",
    "\n",
    "\n",
    "  k = 0\n",
    "  while len(F[k]) != 0:\n",
    "\n",
    "    # We get the new pairs.\n",
    "    generated = generate_unions(F[k])\n",
    "\n",
    "    # We remove those that contain subsets that are not frequent.\n",
    "    pruned = [i for i in generated if prune_subset(F[k], i)]\n",
    "\n",
    "    if k == 0:\n",
    "      pruned = [i for i in pruned if bit_mask[hash_function(list(i)[0],list(i)[1])]]\n",
    "    # We remove those with small support.\n",
    "    with_support = [i for i in pruned if support(i, X) > threshold]\n",
    "\n",
    "    # We add the new pairs.\n",
    "    F.append(with_support)\n",
    "    k += 1\n",
    "  return functools.reduce(lambda a,b: [*a, *b], F)\n",
    "PCY(X, 0.3, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hashing Speedups\n",
    "\n",
    "Hashing speedups are a technique used to improve the efficiency of frequent itemset mining algorithms. Hashing allows frequent itemsets to be stored in a compact form, reducing the memory required to store them and the time required to search for them. This can be particularly useful when dealing with large datasets, as it allows frequent itemsets to be efficiently identified and used to generate association rules."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
